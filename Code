Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FQaUeulrsJWxgDy5GbQYNZEWwG4TDrD-

The problem I am solving is providing valuable information for individuals and organizations who are interested
in investing or working within specific industries. By understanding which industries are the most profitable,
individuals and organizations can make informed decisions about where to allocate their time, money, and
resources.

To do that i am going to be using the data files data.csv and constituent-financials.csv which you can get the original from this link
https://datahub.io/core/s-and-p-500 and
https://datahub.io/core/s-and-p-500-companies-financials#resource-constituents

This dataset contains financial information for each of the 500 companies in the S&P 500 index, including
metrics such as market capitalization, price-to-earnings ratio, and earnings per share. The data is organized in
a tabular format, with one row per company and multiple columns representing various financial metrics.
The size of the dataset is relatively small, with 506 rows (including a header row) and 14 columns. The data
is in CSV (Comma-Separated Values) format, which is a common data format used for tabular data. The
features in the dataset are all numerical and represent various financial metrics that can be used to calculate
profitability.
While the other one include 1768 rows and 10 columns

One change i had to make was that i added a new S&P index fund Data.

Others should care about this idea because it provides important insights into the financial
performance of various industries, which can have a significant impact on the economy and society
as a whole. This information can be used to guide investments, and create opportunities for economic
growth.

I chose this problem because i believe that there is value in understanding which industries are the
most profitable, and that this information can be used to make more informed decisions about
investing and working within specific industries.

One specific hypothesis could be that the technology industry and Real Estate generates the highest
profits, given its rapid growth and significant impact on society. We need to gather and analyze data
to confirm this hypothesis, and to identify which industries actually generate the most profit

There are multiple Research questions such as 

Which companies are the most profitabale? 

Are the Real Estate and IT sector really Profitable?

Which Sector is most profitable?

How well does a Random Forest classifier
perform in predicting the sector of S&P
500 companies based on their financial
features?

How accurate the S&P 500 index
data is?

We will first be cleaning all the data 

Before analysis, the data is
cleaned by removing companies with
zero dividends and replacing 0 values
with median values.
The top 50 companies with the
highest earnings were selected and
displayed on a graph.
Real Estate and IT sectors were most
profitable and their data was
displayed to show how the
companies are doing.
The S&P 500 index fund's data was
also displayed on a graph to show
how it is growing.
Everything else is in comments in the code.
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.impute import SimpleImputer
from scipy import stats
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Load the data into a DataFrame
df = pd.read_csv('constituents-financials_csv.csv')

# Check for missing values
print(df.isnull().sum())
print()

sectors = df['Sector'].unique()
print(sectors)

# Check for missing values in all columns
if df.isnull().sum().sum() > 0:
    # Impute missing values with the mean for all numeric columns
    df.fillna(df.select_dtypes(include=['number']).mean(), inplace=True)

# Replace 0 values with the median for all numeric columns except Dividend Yield
numeric_cols = df.select_dtypes(include=['number']).columns
numeric_cols_except_dividend_yield = numeric_cols.drop('Dividend Yield')
df[numeric_cols_except_dividend_yield] = df[numeric_cols_except_dividend_yield].replace(0, df[numeric_cols_except_dividend_yield].median())

# Print the DataFrame
#print(df)

# Filter the rows where the Dividend Yield is 0
zero_dividend = df.loc[df['Dividend Yield'] == 0]

# Display the Symbol column for those rows
#print(zero_dividend['Symbol'])
#print()
# Drop the rows with zero dividend from the DataFrame
df = df.drop(zero_dividend.index)
# Save the modified DataFrame to a new CSV file
df.to_csv('modified_constituents-financials_csv.csv', index=False)

# Read the modified CSV file into a DataFrame
df1 = pd.read_csv('modified_constituents-financials_csv.csv')

# Print the DataFrame to the console
print(df.isnull().sum())

# Select the numeric columns to normalize
numeric_cols = ['Price', 'Price/Earnings', 'Dividend Yield', 'Earnings/Share',
                '52 Week Low', '52 Week High', 'Market Cap', 'EBITDA',
                'Price/Sales', 'Price/Book']

# Initialize the StandardScaler
scaler = StandardScaler()

# Normalize the numeric columns
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Print the first 5 rows of the normalized DataFrame
print()
print()
print(df.head())

# Sort the DataFrame by Earnings/Share in descending order
sorted_df = df.sort_values(by='Earnings/Share', ascending=False)

# Select the top 10 companies with highest earnings
top_10 = sorted_df.head(10)

# Display the symbol and earnings/share columns for the top 10 companies
print()
print()
print(top_10[['Symbol', 'Earnings/Share']])

# Drop the rows where the Dividend Yield is 0
df = df[df['Dividend Yield'] != 0]

# Sort the DataFrame by Earnings/Share in descending order
df = df.sort_values(by='Earnings/Share', ascending=False)

# Keep only the top 50 rows
df = df.head(50)

# Save the modified DataFrame to a new CSV file
df.to_csv('top_50_highest_earnings.csv', index=False)

# Read the modified CSV file into a DataFrame
df1 = pd.read_csv('top_50_highest_earnings.csv')

# Print the DataFrame to the console
print()
print(df1)

# Load the data into a DataFrame
df = pd.read_csv('modified_constituents-financials_csv.csv')

# Filter the DataFrame to include only tech companies
tech_companies = df[df['Sector'] == 'Information Technology']

# Plot the earnings of the tech companies
plt.plot(tech_companies['Earnings/Share'])

# Add a title and axis labels
plt.title('Earnings of Tech Companies')
plt.xlabel('Company')
plt.ylabel('Earnings/Share')

# Show the plot
plt.show()

# Filter the DataFrame to include only the real estate sector
real_estate_df = df[df['Sector'] == 'Real Estate']

# Group the data by company name and calculate the average earnings per share
real_estate_earnings = real_estate_df.groupby('Name')['Earnings/Share'].mean()

# Sort the data by earnings per share in descending order
real_estate_earnings = real_estate_earnings.sort_values(ascending=False)

# Create a bar plot of the earnings per share for each company
real_estate_earnings.plot(kind='bar', figsize=(10, 5), title='Real Estate Sector Earnings per Share')

"""In the following result we were able to see That the Medical feild was the most profitable in the past year that might be cause if covid and the graphs show us which companis in the real estate are most profitable. And how well did the IT sector did

To clean this data i removed all the companies that have zero dividends than we replaced all the 0 values with the median values to fill the gaps than we picked the top 50 companies with the higest earnings.
"""

# Load the data into a DataFrame
df = pd.read_csv('data_csv.csv')


# Check for missing values
print(df.isnull().sum())
print()

# Remove duplicate rows based on all columns
df = df.drop_duplicates()

# Print the shape of the DataFrame to see the number of rows and columns
print(df.shape)

# Remove all rows that have a 0 value in any column
df = df.loc[(df != 0).all(axis=1)]

# Round all the values to 2 decimal places
df = df.round(2)

# Print the updated DataFrame
#print(df)

# Convert the 'Date' column to a datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Set the 'Date' column as the index
df.set_index('Date', inplace=True)

# Resample the data by year and calculate the mean for each year
df_mean = df.resample('Y').mean()
print(df_mean)

# Plot the 'Interest Rate' column
df['Long Interest Rate'].plot(figsize=(10, 5))

# Add title and axis labels
plt.title('Interest Rate Over Time')
plt.xlabel('Year')
plt.ylabel('Long Interest Rate')

# Show the plot
plt.show()  

# # Save the modified DataFrame to a new CSV file
# df.to_csv('modified_data.csv', index=False)

# Resample the data by year and calculate the mean for each year, including the 'Date' column
df_mean = df.reset_index().set_index('Date').resample('Y').mean().reset_index()
print(df_mean)

# Save the new DataFrame to a CSV file
df_mean.to_csv('yearly_averages.csv', index=False)

# Load the data into a DataFrame
df = pd.read_csv('yearly_averages.csv')
print(df.columns)
# Replace all null values with zero
df = df.fillna(0)

# Set the 'Date' column as the index
df.set_index('Date', inplace=True)

# Normalize the data using Min-Max normalization
df_norm = (df - df.min()) / (df.max() - df.min())

# Round the normalized data to 2 decimal places
df_norm = df_norm.round(decimals=2)

# Save the modified DataFrame to a new CSV file
df_norm.to_csv('normalized_yearly_averages.csv')

# Replace all the null values with zero
df.fillna(0, inplace=True)

# Plot the 'Interest Rate' column
df['Long Interest Rate'].plot(figsize=(10, 5))

# Add title and axis labels
plt.title('Interest Rate Over Time')
plt.xlabel('Year')
plt.ylabel('Long Interest Rate')

# Show the plot
plt.show()

"""This graph shows us the intrest rate thru the years.

Now we do the ML part we do Random forest for classification and Profect for prediction.
This classification report provides a
summary of the performance of the
classifier on the test set, including metrics
such as precision, recall, and F1-score, for
each class in the target variable. So, the
question that can be solved with this is:
How well does a Random Forest classifier
perform in predicting the sector of S&P
500 companies based on their financial
features? How accurate the S&P 500 index
data is?
"""

df = pd.read_csv('modified_constituents-financials_csv.csv')

# Split the data into features (X) and target (y)
X = df.drop('Sector', axis=1) 
y = df['Sector'] 

# One-hot encode the categorical data in the feature matrix X
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X)

# Split the encoded data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)


rfc = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rfc.fit(X_train, y_train)
y_pred = rfc.predict(X_test)


print(classification_report(y_test, y_pred))

import pandas as pd

df = pd.read_csv('normalized_yearly_averages.csv')

# Convert date column to numeric value
df['Date'] = pd.to_datetime(df['Date']).apply(lambda x: x.timestamp())

# Split the data into features (X) and target (y)
X = df.drop('Earnings', axis=1) 
y = df['Earnings'] 

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a random forest regressor model
from sklearn.ensemble import RandomForestRegressor
rfc = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
rfc.fit(X_train, y_train)
y_pred = rfc.predict(X_test)

# Evaluate the model
from sklearn.metrics import r2_score
print('R2 score:', r2_score(y_test, y_pred))

"""Now the second model Prophet Time series task
To forecast the future prices, I used the make_future_dataframe() function to generate a dataframe
with future dates. I set the number of periods to 3650, which corresponds to 10 years.
I used the predict() function of the Prophet model to make predictions on the future dataframe.
I then plotted the forecasted values using the plot() function of the Prophet model. I set the xlabel to
'Date' and the ylabel to 'Price' to label the plot axes
Based on the time series forecasting, we were able to predict the future prices of the S&P 500 index
for the next 10 years. The plot shows the historical and forecasted values, along with the upper and
lower bounds of the prediction intervals.
The Research question that is answered is that Is investing in the s&p 500 safe for the next 10 years 

"""

!pip install pystan~=2.14
!pip install fbprophet

import pandas as pd
import matplotlib.pyplot as plt
from fbprophet import Prophet

df = pd.read_csv('normalized_yearly_averages.csv')
df['Date'] = pd.to_datetime(df['Date'])
df = df.rename(columns={'Date': 'ds', 'SP500': 'y'})
model = Prophet()
model.fit(df)
future = model.make_future_dataframe(periods=3650) # 10 years
forecast = model.predict(future)
model.plot(forecast, xlabel='Date', ylabel='Price')

"""Conclusion/Lessons Learn
The main goal of the project was to show
Where to invest after all the data we saw
we can conclude that we can invest in the
S&P index fund that is the best option other
than that we have a really good chance to
invest in the Real Estate and the IT sector
and be profitable. 
